install.packages('randomForest')
install.packages('BART')
install.packages('gbm')
knitr::opts_chunk$set(error = TRUE)
library(tree)
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
Carseats <- data.frame(Carseats, High)
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.carseats
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats,
subset = train)
tree.pred <- predict(tree.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
(104 + 50) / 200
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
(97 + 58) / 200
prune.carseats <- prune.misclass(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test,
type = "class")
table(tree.pred, High.test)
(102 + 52) / 200
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, importance = TRUE)
bag.boston
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
bag.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian", n.trees = 5000,
interaction.depth = 4, shrinkage = 0.2, verbose = F)
yhat.boost <- predict(boost.boston,
newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
library(BART)
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
knitr::opts_chunk$set(error = TRUE)
library(ISLR2)
set.seed(1)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
cv.error <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
alpha.fn(Portfolio, 1:100)
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
boot(Portfolio, alpha.fn, R = 1000)
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
boot(Auto, boot.fn, 1000)
summary(lm(mpg ~ horsepower, data = Auto))$coef
boot.fn <- function(data, index)
coef(
lm(mpg ~ horsepower + I(horsepower^2),
data = data, subset = index)
)
set.seed(1)
boot(Auto, boot.fn, 1000)
summary(
lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
)$coef
knitr::opts_chunk$set(error = TRUE)
library(ISLR2)
set.seed(1)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
cv.error <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
alpha.fn <- function(data, index) {
X <- data$X[index]
Y <- data$Y[index]
(var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
alpha.fn(Portfolio, 1:100)
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
boot(Portfolio, alpha.fn, R = 1000)
boot.fn <- function(data, index)
coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
boot(Auto, boot.fn, 1000)
summary(lm(mpg ~ horsepower, data = Auto))$coef
boot.fn <- function(data, index)
coef(
lm(mpg ~ horsepower + I(horsepower^2),
data = data, subset = index)
)
set.seed(1)
boot(Auto, boot.fn, 1000)
summary(
lm(mpg ~ horsepower + I(horsepower^2), data = Auto)
)$coef
library(ggplot2)
# variables
# The diameter of the Milky Way is 185000 light years
# https://beltoforion.de/en/drake-equation/drake-equation.php#idRef7
R_M = 92500 # light years
f_p = 1   #  - Proportion of stars with planets
n_e = 0.4 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 0.1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.001 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.1#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
cat(L, "  years")
print(L)
cat("************************\n")
cat("Scenario 1  \n")
cat(L, "  years \n ")
cat("************************\n")
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
#cat("************************\n")
print("Scenario 1  \n")
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
f_p = 1   #  - Proportion of stars with planets
n_e = 0.4 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.5 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.5#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
print("Scenario 2  \n")
print(L)
print("  years \n ")
f_p = 0.1   #  - Proportion of stars with planets
n_e = 0.001 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 0.1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.0000000001 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.0000000001#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
print("Scenario 3  \n")
print(L)
print("  years \n ")
9432054457/1e6
9432054457/1e9
source("~/soumya_cam_mac/cosmic_loneliness_equation/drake_generalization.R", echo=TRUE)
library(ggplot2)
# variables
# The diameter of the Milky Way is 185000 light years
# https://beltoforion.de/en/drake-equation/drake-equation.php#idRef7
R_M = 92500 # light years
f_p = 1   #  - Proportion of stars with planets
n_e = 0.4 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 0.1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.001 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.1#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
f_p = 1   #  - Proportion of stars with planets
n_e = 0.4 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.5 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.5#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
# Parameters
mass <- 1  # Mass of the oscillator (kg)
spring_constant <- 1  # Spring constant (N/m)
initial_displacement <- 0.5  # Initial displacement (m)
initial_velocity <- 0  # Initial velocity (m/s)
time <- seq(0, 10, by = 0.01)  # Time array (s)
# Function to calculate displacement and velocity
harmonic_oscillator <- function(time, mass, spring_constant, initial_displacement, initial_velocity) {
angular_frequency <- sqrt(spring_constant / mass)  # Angular frequency (rad/s)
displacement <- initial_displacement * cos(angular_frequency * time) +
(initial_velocity / angular_frequency) * sin(angular_frequency * time)
velocity <- -initial_displacement * angular_frequency * sin(angular_frequency * time) +
initial_velocity * cos(angular_frequency * time)
return(list(time = time, displacement = displacement, velocity = velocity))
}
# Simulate the motion
motion <- harmonic_oscillator(time, mass, spring_constant, initial_displacement, initial_velocity)
# Plot displacement vs time
plot(motion$time, motion$displacement, type = "l",
xlab = "Time (s)", ylab = "Displacement (m)",
main = "Simple Harmonic Oscillator: Displacement vs Time")
# Plot velocity vs time
plot(motion$time, motion$velocity, type = "l",
xlab = "Time (s)", ylab = "Velocity (m/s)",
main = "Simple Harmonic Oscillator: Velocity vs Time")
data(heart)
source("~/.active-rstudio-document", echo=TRUE)
version
source("~/.active-rstudio-document", echo=TRUE)
install.packages('lindenmayeR')
install.packages('LindenmayeR')
library(LindenmayeR)
islands_rules <- data.frame(inp = c("F", "f"), out = c("F+f-FF+F+FF+Ff+FF-f+FF-F-FF-Ff-FFF",
"ffffff"), stringsAsFactors = FALSE)
islands <- Lsys(init = "F+F+F+F", rules = islands_rules, n = 2)
draw_islands <- data.frame(symbol = c("F", "f", "+", "-", "[", "]"),
action = c("F", "f", "+", "-", "[", "]"), stringsAsFactors = FALSE)
drawLsys(string = islands, step = 1, ang = 90, st = c(70, 35, 90),
drules = draw_islands,  gp = gpar(col = "red", fill = "gray"))
islands_rules <- data.frame(inp = c("F", "f"), out = c("F+f-FF+F+FF+Ff+FF-f+FF-F-FF-Ff-FFF",
"ffffff"), stringsAsFactors = FALSE)
islands <- Lsys(init = "F+F+F+F", rules = islands_rules, n = 2)
draw_islands <- data.frame(symbol = c("F", "f", "+", "-", "[", "]"),
action = c("F", "f", "+", "-", "[", "]"), stringsAsFactors = FALSE)
drawLsys(string = islands, step = 1, ang = 90, st = c(70, 35, 90),
drules = draw_islands,  gp = gpar(col = "red", fill = "gray"))
##' # A more realistic botanical structure
##' # Some call this a fractal tree, I think it is more like seaweed
##' # Try drawing the last iteration (too slow for here, but looks great)
fractal_tree_rules <- data.frame(inp = c("X", "F"),
out = c("F-[[X]+X]+F[+FX]-X", "FF"), stringsAsFactors = FALSE)
fractal_tree <- Lsys(init = "X", rules = fractal_tree_rules, n = 7)
draw_ft <- data.frame(symbol = c("X", "F", "+", "-", "[", "]"),
action = c("f", "F", "+", "-", "[", "]"), stringsAsFactors = FALSE)
drawLsys(string = fractal_tree, stepSize = 2, ang = 25, st = c(50, 5, 90),
drules = draw_ft, which = 5, gp = gpar(col = "chocolate4", fill = "honeydew"))
grid.text("Fractal Seaweed (n = 4)", 0.25, 0.25)
islands_rules <- data.frame(inp = c("F", "f"), out = c("F+f-FF+F+FF+Ff+FF-f+FF-F-FF-Ff-FFF",
"ffffff"), stringsAsFactors = FALSE)
islands <- Lsys(init = "F+F+F+F", rules = islands_rules, n = 2)
draw_islands <- data.frame(symbol = c("F", "f", "+", "-", "[", "]"),
action = c("F", "f", "+", "-", "[", "]"), stringsAsFactors = FALSE)
drawLsys(string = islands, step = 1, ang = 90, st = c(70, 35, 90),
drules = draw_islands,  gp = gpar(col = "red", fill = "gray"))
source("~/.active-rstudio-document", echo=TRUE)
if = 9
c(8,9)
c(8,9,'j')
sum(c(30,'hello'))
height = c(30,40,50)
height[3]
height[4]
height[c(3,2)]
source("~/.active-rstudio-document", echo=TRUE)
library(igraph)
# Create a weighted graph
edges <- c("A", "B", 1,
"A", "C", 2,
"B", "C", 3,
"B", "D", 4,
"C", "D", 5)
# Build the graph object
g <- graph_from_data_frame(as.data.frame(matrix(edges, ncol = 3, byrow = TRUE)),
directed = FALSE)
# Compute Kruskal's MST
mst <- mst(g, algorithm = "kruskal")
# Compute Kruskal's MST
mst <- mst(g)
# Display the MST
plot(mst, edge.label = E(mst)$weight)
library(ggplot2)
# variables
# The diameter of the Milky Way is 185000 light years
# https://beltoforion.de/en/drake-equation/drake-equation.php#idRef7
R_M = 46250 # light years
f_p = 1   #  - Proportion of stars with planets
n_e = 0.4 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 0.1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.001 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.1#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
print("Scenario 1  \n")
print(L)
print("  years \n ")
f_p = 1   #  - Proportion of stars with planets
n_e = 0.4 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.5 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.5#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
print("Scenario 2  \n")
print(L)
print("  years \n ")
f_p = 0.1   #  - Proportion of stars with planets
n_e = 0.001 #  - Average number of planets per star on which life is possible
# After observations of exoplanets with the Kepler Space Telescope, researchers estimate that there may be 40 billion Earth-like planets in the Milky Way in the habitable zone of Sun-like stars or red dwarfs. This suggests that the product of
# and  is about 0.4:
f_l = 0.1 #  - Proportion of planets that could support life and on which life actually develops.
f_i = 0.0000000001 #  - Proportion of planets with life on which intelligent life develops.
f_c = 0.0000000001#  - Proportion of civilizations that send detectable signals into space.
L = R_M^(3/4) / ( (f_p * n_e * f_l * f_i * f_c)^(1/4) )
print(L)
print("Scenario 3  \n")
print(L)
print("  years \n ")
5608333135/10^9
setwd("~/soumya_cam_mac/teaching/teaching_unsupervised_machine_learning_private/practical")
set.seed(2)
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
km.out=kmeans(x,2,nstart=20)
km.out$cluster
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)
x
set.seed(2)
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
km.out=kmeans(x,2,nstart=20)
km.out$cluster
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)
set.seed(4)
km.out=kmeans(x,3,nstart=20)
km.out
plot(x, col=(km.out$cluster+1), main="K-Means Clustering Results with K=3", xlab="", ylab="", pch=20, cex=2)
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
?kmeans
x=matrix(rnorm(50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
head(x)
x=matrix(rnorm(50*2), ncol=2)
head(x)
km.out=kmeans(x,centers=2,nstart=20)
km.out$cluster
plot(x)
plot(x, col=(km.out$cluster), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)
km.out=kmeans(x,clusters=3,nstart=20)
?kmeans
km.out=kmeans(x,centers=3,nstart=20)
km.out
plot(x, col=(km.out$cluster), main="K-Means Clustering Results with K=3", xlab="", ylab="", pch=20, cex=2)
km.out$tot.withinss
set.seed(3)
km.out=kmeans(x,centers=3,nstart=1)
set.seed(3)
km.out=kmeans(x,centers=3,nstart=1)
km.out$tot.withinss
plot(x, col=(km.out$cluster). main = "K-Means clustering results with K=2 and nstart=1", xlab="", ylab="", pch=20, cex=2)
set.seed(3)
km.out=kmeans(x,centers=3,nstart=1)
set.seed(3)
km.out=kmeans(x,centers=3,nstart=1)
km.out$tot.withinss
plot(x, col=(km.out$cluster), main = "K-Means clustering results with K=2 and nstart=1", xlab="", ylab="", pch=20, cex=2)
km.out=kmeans(x,centers=3,nstart=20)
km.out$tot.withinss
plot(x, col=(km.out$cluster), main = "K-Means clustering results with K=2 and nstart=20", xlab="", ylab="", pch=20, cex=2)
?kmeans
